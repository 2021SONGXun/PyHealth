# -*- coding: utf-8 -*-"""MIMIC dataset handling using parallelization on demo data"""# Author: Yue Zhao <zhaoy@cmu.edu># License: BSD 2 clauseimport osimport sysimport pandas as pdimport jsonfrom joblib import Parallel, delayed# temporary solution for relative imports in case combo is not installed# if combo is installed, no need to use the following linesys.path.append(    os.path.abspath(os.path.join(os.path.dirname("__file__"), '..')))from pyhealth.data.base_mimic import parallel_parse_tablesfrom pyhealth.utils.utility_parallel import unfold_parallelfrom pyhealth.utils.utility_parallel import partition_estimatorsfrom pyhealth.utils.utility import read_csv_to_dffrom pyhealth.utils.utility import make_dirs_if_not_existsimport warningsif not sys.warnoptions:    warnings.simplefilter("ignore")if __name__ == "__main__":    n_jobs = 4  # number of parallel jobs    duration = 21600  # time window for episode generation    selection_method = 'last'    save_dir = os.path.join('outputs', 'mimic_demo', 'raw')    make_dirs_if_not_exists(save_dir)    patient_data_loc = os.path.join(save_dir, 'patient_data_demo.json')    valid_data_list = []  # keep tracking the stored data    valid_id_list = []  # keep tracking a list of patient IDs    # key variables to track in the episode    var_list = ['Capillary refill rate',                'Diastolic blood pressure',                'Fraction inspired oxygen',                'Glascow coma scale eye opening',                'Glascow coma scale motor response',                'Glascow coma scale total',                'Glascow coma scale verbal response',                'Glucose',                'Heart Rate',                'Height',                'Mean blood pressure',                'Oxygen saturation',                'Respiratory rate',                'Systolic blood pressure',                'Temperature',                'Weight',                'pH']    # enforce and convert to lower case    var_list = [item.lower() for item in var_list]    #################################################################    # read in tables    patient_df = read_csv_to_df(        os.path.join('data', 'mimic-iii-clinical-database-demo-1.4',                     'PATIENTS.csv'))    patient_id_list = patient_df['subject_id'].tolist()    admission_df = read_csv_to_df(        os.path.join('data', 'mimic-iii-clinical-database-demo-1.4',                     'ADMISSIONS.csv'))    icu_df = read_csv_to_df(        os.path.join('data', 'mimic-iii-clinical-database-demo-1.4',                     'ICUSTAYS.csv'))    cevent_df = read_csv_to_df(        os.path.join('data', 'mimic-iii-clinical-database-demo-1.4',                     'CHARTEVENTS.csv'), low_memory=False)    cevent_s_df = cevent_df[['subject_id',                             'hadm_id',                             'icustay_id',                             'itemid',                             'charttime',                             'cgid',                             'value',                             'valueuom', ]]    oevent_df = read_csv_to_df(        os.path.join('data', 'mimic-iii-clinical-database-demo-1.4',                     'OUTPUTEVENTS.csv'), low_memory=False)    oevent_s_df = oevent_df[['subject_id',                             'hadm_id',                             'icustay_id',                             'itemid',                             'charttime',                             'cgid',                             'value',                             'valueuom', ]]    event_df = pd.concat([cevent_s_df, oevent_s_df])    event_df['charttime'] = pd.to_datetime(event_df['charttime'])    event_mapping_df = read_csv_to_df(        os.path.join('resources', 'itemid_to_variable_map.csv'))    event_mapping_df['level2'] = event_mapping_df['level2'].str.lower()    ##############################################################################    # parallel data processing starts    n_patients_list, starts, n_jobs = partition_estimators(        len(patient_id_list), n_jobs=n_jobs)    all_results = Parallel(n_jobs=n_jobs, max_nbytes=None, verbose=True)(        delayed(parallel_parse_tables)(            patient_id_list=patient_id_list[starts[i]:starts[i + 1]],            patient_df=patient_df,            admission_df=admission_df,            icu_df=icu_df,            event_df=event_df,            event_mapping_df=event_mapping_df,            duration=duration,            selection_method=selection_method,            var_list=var_list,            save_dir=save_dir)        for i in range(n_jobs))    all_results = list(map(list, zip(*all_results)))    valid_data_list = unfold_parallel(all_results[0], n_jobs)    valid_id_list = unfold_parallel(all_results[1], n_jobs)    patient_data_list = []    for p in valid_data_list:        patient_data_list.append(p.data)    with open(patient_data_loc, 'w') as outfile:        json.dump(patient_data_list, outfile)